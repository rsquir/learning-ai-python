https://developers.google.com/machine-learning/crash-course/representation/feature-engineering


11111
** Framing: Key ML Technology
Supervised machine learning is ML systems learning how to combine input to produce predictions on never-before-seen data
Labels are y in a linear regression
Features are x in a linear regression
Examples are an instance of data, x
labeled examples include both feature and label
unlabeled examples include feature but not label
Models:
    training/learning, takes examples to build relationships 
between features and labels
    inference means applying trained model to unlabeled examples
Regression predicts continuous values "What is the value of a house in California?/What is the probability that a user will click on this ad?"
classification predicts discrete values "Is a given email message spam or not spam?/Is this an image of a dog, a cat, or a hamster?"


22222
** Descending into ML
y = wx + b
y is the predicted label
b is the bias
w is the weight of feature 1
x is a feature
to infer substitute x with a value
y = b + w1x1 + w2x2 + w3x3


** Descending into ML: Training and Loss
training a model means learning (determining) good values for all the weights and bias from labeled examples supervised learning means it has examples and then tries to minimize loss, empirical risk minimization
loss is the penalty for a bad prediction, a nuber indicating for bad the model's prediction was on a single example
perfection is 0 else greater
training is to find a set of weights and biases that have low loss on average across all examples
squared loss (aka l2 loss) is (observation - prediction(x)) ** 2
Mean Squared Error (mse) is the average loss per example over an entire dataset
1/n * while i < n: mse(x, y)
x are features
y is label


33333
** Reducing Loss
convergence means iterating loss until it stops or becomes very slow


** Reducing Loss: Gradient Descent
convex problems have only one minimum (where slope is 0)
gradient descent, pick a starting value for w (0 or random) gradient descent calculates the gradient of the loss curve at the starting point (curve or slope ie derivative) which tells you if you're hotter or colder
gradient is a vector of partial derivatives with respect to the weights
gradient is a vector so [direction, magnatude]
the gradient points to the steepest increases in loss, the gradient descent algorithm takes a step in the negative gradient to reduce loss


** Reducing Loss: Learning Rate
learning rate (step size) is used to determine the next point
hyperparameters are the values we tweak
learning rate too small it will take a while
learning rate too big it won't resolve (it'll bounce around the end)
find goldilocks for optimum learning rate

** Reducing Loss: Stochastic Gradient Descent
batch is number of examples used to calculate gradient in a single iteration
Stochastic gradient descent (SGD) makes this idea extreme, batch size of 1
mini-batch stochastic gradient descent (mini-batch SDG) is between 10 and 1000 batches


44444
** TensorFlow
sequence_of_integers = np.arange(5, 12)
random_integers_between_50_and_100 = np.random.randint(low=50, high=101, size=(6)) # Note high is +1 over wanted range
random_floats_between_0_and_1 = np.random.random([6]) # idk why square brackets
random_floats_between_2_and_3 = random_floats_between_0_and_1 + 2.0 # broadcasting is the + 2.0, which adds 2.0 to each vector

my_dataframe = pd.DataFrame(data=my_data, columns=my_column_names)
my_dataframe["adjusted"] = my_dataframe["activity"] + 2
print("Rows #0, #1, and #2:")
print(my_dataframe.head(3), '\n')
print("Row #2:")
print(my_dataframe.iloc[[2]], '\n')
print("Rows #1, #2, and #3:")
print(my_dataframe[1:4], '\n')
print("Column 'temperature':")
print(my_dataframe['temperature'])


55555
** Generalization
Generalization refers to your model's ability to adapt to new, previously unseen data, drawn from the same distribution as the one used to create the model

** Peril of Overfitting
an overfit model gets a low loss during training but does a poor job predicting data
over complicated
objective is between fitting our data and also fitting the data as simply as possible
training set
test set
we draw examples independently and identically (i.i.d) at random from the distribution (ie. examples don't influence eachother--iid=randomness of variables)
stationary distribution and same distribution


666666
** Training and Test Sets: Splitting Data
training set is a subset to train a model
test set is a subset to test the trained model
split data say, 90:10 for train:test
batch doesn't effect loss


77777
** Validation Set 
using just test data could overfit
validation data is used to train the training data (keep test data off on the side)
final test on test data, looking for a match on validation data
Tweak model, divide data into 3 sets: training, validation, test
test is untouched, most tweaking done in validation set
test is untouched because it would train the data rendering it useless


** Validation Sets and Test Sets: Programming Exercise
shuffled_train_df = train_df.reindex(np.random.permutation(train_df.index))


88888
** Representation: Feature Engineering
feature engineering is transforming raw data into a feature vector
raw data = { num_rooms: 6, num_bedrooms: 3 }
feature vector = [6.0, 3.0]
int to float is a trivial conversion
catagorical features have a discrete (distinct) set of possible values, example 'Charleston Road', 'North Shoreline Boulevard'
vocabulary possible values to integers, (OOV bucket:) map = Charleston Road to 0, North Shoreline Boulevard to 1
	vocabulary with int poses problems with weight, for example 0 * 6 (0 is Charleston Road, 6 is weight)
in mapping example some addresses are on the corner of a st and ave
**(need to know more) one-hot encoding assigns the desired string to binary 1, others 0, multi-hot does two binary 1's as its values
one-hot isn't feasable for millions of elements so we use sparse representation (stores a 1 in all words in a sentence, dog tail wag, every word in sentence is 1 rest in the df is 0)


** Qualities of Good Features
Good feature values should appear more than 5 times in a data set
can't learn from ids example, unique_house_id: 8SK982ZZ1242Z
good naming conventions
homebrew values example, house_age: 851472000
noisy data example, user_age_years: 277
!"magic" values example, good=quality_rating: 0.37, conversly bad=quality_rating: -1 ("magic")
use quality_rating, is_quality_rating_define to mark magic values
strong data that won't change ei. city names
accounting for other ids: inferred_city_cluster: "219"


** Cleaning Data
scaling, reduce to bounds -1, +1; 0 to 1

scaled_value = (value - mean) / stddev

(mean = 100
 standard deviation = 20
 original value = 130)
scaled_value = (130 - 100) / 20
scaled_value = 1.5

roomsPerPerson clipping (if max is 4.0 all values over 4 are grouped so expect a spike at the end 'removing outliers')
you can log((totalRooms / population) + 1), don't know what this does or how it works
binning for latitude, reduced to 11 booleans, most value is 1 rest are 0
quantile binning, same number of points 'points' in each bin

scrubbing (omitted values, forgetful user; duplicate examples, server duplicate; bad labels, mislabeled a picture; bad feature vector, thermometer left out in sun)

max and min, median and mean, standard deviation for visualizing data


99999
** Feature Crosses
feature cross is a synthetic feature (formed by multiplying, crossing, two or more features)
feature crosses [A x B], [A x B x C x D]
when binary can be sparse (lots of zeros)
example [latitude x num_bedrooms]
linear learnings and deep neural networks scale very well

** Feature Crosses: Encoding Nonlinearity
x3 = x1 * x2
y = b + w1x1 + w2x2 + w3x3
[A x B] feature multiplying the values of two features
[A x B x C x D x E] feature cross formed by multiplying values of 5 features
[A x A] a feature cross y squaring the one feature

** Feature Crosses: Crossing One-Hot Vectors
ml seldom cross continuous features
ml do frequently cross one-hot features
one-hot:
    country=USA, country=France
    language=English, language=Spanish
    country:usa AND language:spanish
bin latitude:
    binned_latitude = [0, 0, 0, 1, 0]
    binned_longitude = [0, 1, 0, 0, 0]
    binned_latitude X binned_longitude
this feature is a 25-element one-hot vector (24 zeroes and 1 one)
    binned_latitude(lat) = [
        0 < lat <= 10
        10 < lat <= 20
        20 < lat <= 30
    ]
    binned_longitude = [
        0 < lon <= 15
        15 < lon <= 30
    ]
creating a cross feature of these is
    binned_latitude_X_longitude(lat, lon) = [
        0 < lat <= 10 AND 0 < lon <= 15
        0 < lat <= 10 AND 15 < lon <= 30
        10 < lat <= 20 AND 0 < lon <= 15
        10 < lat <= 20 AND 15 < lon <= 30
        20 < lat <= 30 AND 0 < lon <= 15
        20 < lat <= 30 AND 0 < lon <= 30
    ]
example two features used usefully to make predictions, dog behaviour (barking, snuggling, etc.) and time of day
[behaviour type X time of day]
    5:00pm dog cries happily (owner home from work)
    3:00am sleeping


1010101010
** Regularization for Simplicity
generalize: not trusting your examples too much
regularization is what we do to avoid overfitting
    1. stop early, stop before on convergint training data
        difficult to do on practice, but often used
    2. penalize model complexity
        a. empirical risk minimization
        b. structural risk minimization
        balance these
define model complexity
    1. prefer smaller weights
        ridge regularization or L2 regularization, penalize the sum of the squared values of the weights
        pays attention to training data but makes sure our weights are sort of not bigger than they need to be
        loss
        λ scalar value that controls how weights are balanced, which means getting the examples right vs/ making the model simple
        w1**2 + ... + wn**2: square of L2 norm


** Regularization for Simplicity: L₂ Regularization
regularization, penalizing complexity
minimize(loss(data|model))
minimize(loss(data|model) + complexity(model))
    this is the loss term and the regularization term
This course focuses on two common ways to think of complexity
    model complexity as a function of weights of all features in the model
    model complexity as a function of the total number of features with nonzero weights
L2 regularization
    sum of the squares of all the feature weights
    w1**2 + w2**2 + w3**2 ...

** Regularization for Simplicity: Lambda
lambda = regularization rate
    l2 regularization has:
        encourages weight values toward 0 (but not exactly 0)
        encouraged the mean of the weights toward 0
increasing lambda strengthens regularization effect
if your lambda value is too high your model will be simple, but you run the risk of underfitting. your model wont learn enough about the training data to make useful predictions
too low and model will be more complex, overfitting
lambda at 0 removes regularizartion
lambda and learning rate have a close connection
strong l2 drives feature weights to 0, lower learning rates (with early stopping) often produce often produce the name effect b/c the steps away from 0 aren't as large
making learning rate/lambda simultaneously have confounding effects

** UI Exercise
with small noisy training sets overfitting is a real concern


1111111111
** Logistic Regression
logistic regressions generates a probability, a value between 0 and 1
if the model says 0.932--93.2% then that much will actually be spam (in the spam example)

** Logistic Regression: Calculating a Probability
sigmoid gives value between 0 and 1
    there's asymptotes so it never hits 0 or 1, because of these we need regularization
if we need non-linearities we can by cross product
p(bar|night)
startled = p(bark|night) * nights
    = 0.05 * 365
    = 18
y' = sigmoid 1 / (1 + e**-z)
z = log(y / 1 - y)

** Logistic Regression: Loss and Regularization
regularization is extremelt important in logistic regression modelling b/c without it the asymptotic nature of logreg would keep driving loss toward 0
most logistic regression models use:
    L2 regularization
    early stopping, limiting number of steps/learning rate



