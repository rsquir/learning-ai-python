https://developers.google.com/machine-learning/crash-course/representation/feature-engineering


11111
** Framing: Key ML Technology
Supervised machine learning is ML systems learning how to combine input to produce predictions on never-before-seen data
Labels are y in a linear regression
Features are x in a linear regression
Examples are an instance of data, x
labeled examples include both feature and label
unlabeled examples include feature but not label
Models:
    training/learning, takes examples to build relationships 
between features and labels
    inference means applying trained model to unlabeled examples
Regression predicts continuous values "What is the value of a house in California?/What is the probability that a user will click on this ad?"
classification predicts discrete values "Is a given email message spam or not spam?/Is this an image of a dog, a cat, or a hamster?"


22222
** Descending into ML
y = wx + b
y is the predicted label
b is the bias
w is the weight of feature 1
x is a feature
to infer substitute x with a value
y = b + w1x1 + w2x2 + w3x3


** Descending into ML: Training and Loss
training a model means learning (determining) good values for all the weights and bias from labeled examples supervised learning means it has examples and then tries to minimize loss, empirical risk minimization
loss is the penalty for a bad prediction, a nuber indicating for bad the model's prediction was on a single example
perfection is 0 else greater
training is to find a set of weights and biases that have low loss on average across all examples
squared loss (aka l2 loss) is (observation - prediction(x)) ** 2
Mean Squared Error (mse) is the average loss per example over an entire dataset
1/n * while i < n: mse(x, y)
x are features
y is label


33333
** Reducing Loss
convergence means iterating loss until it stops or becomes very slow


** Reducing Loss: Gradient Descent
convex problems have only one minimum (where slope is 0)
gradient descent, pick a starting value for w (0 or random) gradient descent calculates the gradient of the loss curve at the starting point (curve or slope ie derivative) which tells you if you're hotter or colder
gradient is a vector of partial derivatives with respect to the weights
gradient is a vector so [direction, magnatude]
the gradient points to the steepest increases in loss, the gradient descent algorithm takes a step in the negative gradient to reduce loss


** Reducing Loss: Learning Rate
learning rate (step size) is used to determine the next point
hyperparameters are the values we tweak
learning rate too small it will take a while
learning rate too big it won't resolve (it'll bounce around the end)
find goldilocks for optimum learning rate

** Reducing Loss: Stochastic Gradient Descent
batch is number of examples used to calculate gradient in a single iteration
Stochastic gradient descent (SGD) makes this idea extreme, batch size of 1
mini-batch stochastic gradient descent (mini-batch SDG) is between 10 and 1000 batches


44444
** TensorFlow
sequence_of_integers = np.arange(5, 12)
random_integers_between_50_and_100 = np.random.randint(low=50, high=101, size=(6)) # Note high is +1 over wanted range
random_floats_between_0_and_1 = np.random.random([6]) # idk why square brackets
random_floats_between_2_and_3 = random_floats_between_0_and_1 + 2.0 # broadcasting is the + 2.0, which adds 2.0 to each vector

my_dataframe = pd.DataFrame(data=my_data, columns=my_column_names)
my_dataframe["adjusted"] = my_dataframe["activity"] + 2
print("Rows #0, #1, and #2:")
print(my_dataframe.head(3), '\n')
print("Row #2:")
print(my_dataframe.iloc[[2]], '\n')
print("Rows #1, #2, and #3:")
print(my_dataframe[1:4], '\n')
print("Column 'temperature':")
print(my_dataframe['temperature'])


55555
** Generalization
Generalization refers to your model's ability to adapt to new, previously unseen data, drawn from the same distribution as the one used to create the model

** Peril of Overfitting
an overfit model gets a low loss during training but does a poor job predicting data
over complicated
objective is between fitting our data and also fitting the data as simply as possible
training set
test set
we draw examples independently and identically (i.i.d) at random from the distribution (ie. examples don't influence eachother--iid=randomness of variables)
stationary distribution and same distribution


666666
** Training and Test Sets: Splitting Data
training set is a subset to train a model
test set is a subset to test the trained model
split data say, 90:10 for train:test
batch doesn't effect loss


77777
** Validation Set 
using just test data could overfit
validation data is used to train the training data (keep test data off on the side)
final test on test data, looking for a match on validation data
Tweak model, divide data into 3 sets: training, validation, test
test is untouched, most tweaking done in validation set
test is untouched because it would train the data rendering it useless


** Validation Sets and Test Sets: Programming Exercise
shuffled_train_df = train_df.reindex(np.random.permutation(train_df.index))


88888
** Representation: Feature Engineering
feature engineering is transforming raw data into a feature vector
raw data = { num_rooms: 6, num_bedrooms: 3 }
feature vector = [6.0, 3.0]
int to float is a trivial conversion
catagorical features have a discrete (distinct) set of possible values, example 'Charleston Road', 'North Shoreline Boulevard'
vocabulary possible values to integers, (OOV bucket:) map = Charleston Road to 0, North Shoreline Boulevard to 1
	vocabulary with int poses problems with weight, for example 0 * 6 (0 is Charleston Road, 6 is weight)
in mapping example some addresses are on the corner of a st and ave
**(need to know more) one-hot encoding assigns the desired string to binary 1, others 0, multi-hot does two binary 1's as its values
one-hot isn't feasable for millions of elements so we use sparse representation (stores a 1 in all words in a sentence, dog tail wag, every word in sentence is 1 rest in the df is 0)


** Qualities of Good Features
Good feature values should appear more than 5 times in a data set
can't learn from ids example, unique_house_id: 8SK982ZZ1242Z
good naming conventions
homebrew values example, house_age: 851472000
noisy data example, user_age_years: 277
!"magic" values example, good=quality_rating: 0.37, conversly bad=quality_rating: -1 ("magic")
use quality_rating, is_quality_rating_define to mark magic values
strong data that won't change ei. city names
accounting for other ids: inferred_city_cluster: "219"


** Cleaning Data
scaling, reduce to bounds -1, +1; 0 to 1

scaled_value = (value - mean) / stddev

(mean = 100
 standard deviation = 20
 original value = 130)
scaled_value = (130 - 100) / 20
scaled_value = 1.5

roomsPerPerson clipping (if max is 4.0 all values over 4 are grouped so expect a spike at the end 'removing outliers')
you can log((totalRooms / population) + 1), don't know what this does or how it works
binning for latitude, reduced to 11 booleans, most value is 1 rest are 0
quantile binning, same number of points 'points' in each bin

scrubbing (omitted values, forgetful user; duplicate examples, server duplicate; bad labels, mislabeled a picture; bad feature vector, thermometer left out in sun)

max and min, median and mean, standard deviation for visualizing data


99999
** Feature Crosses
feature cross is a synthetic feature (formed by multiplying, crossing, two or more features)
feature crosses [A x B], [A x B x C x D]
when binary can be sparse (lots of zeros)
example [latitude x num_bedrooms]
linear learnings and deep neural networks scale very well

** Feature Crosses: Encoding Nonlinearity
x3 = x1 * x2
y = b + w1x1 + w2x2 + w3x3
[A x B] feature multiplying the values of two features
[A x B x C x D x E] feature cross formed by multiplying values of 5 features
[A x A] a feature cross y squaring the one feature

** Feature Crosses: Crossing One-Hot Vectors
ml seldom cross continuous features
ml do frequently cross one-hot features
one-hot:
    country=USA, country=France
    language=English, language=Spanish
    country:usa AND language:spanish
bin latitude:
    binned_latitude = [0, 0, 0, 1, 0]
    binned_longitude = [0, 1, 0, 0, 0]
    binned_latitude X binned_longitude
this feature is a 25-element one-hot vector (24 zeroes and 1 one)
    binned_latitude(lat) = [
        0 < lat <= 10
        10 < lat <= 20
        20 < lat <= 30
    ]
    binned_longitude = [
        0 < lon <= 15
        15 < lon <= 30
    ]
creating a cross feature of these is
    binned_latitude_X_longitude(lat, lon) = [
        0 < lat <= 10 AND 0 < lon <= 15
        0 < lat <= 10 AND 15 < lon <= 30
        10 < lat <= 20 AND 0 < lon <= 15
        10 < lat <= 20 AND 15 < lon <= 30
        20 < lat <= 30 AND 0 < lon <= 15
        20 < lat <= 30 AND 0 < lon <= 30
    ]
example two features used usefully to make predictions, dog behaviour (barking, snuggling, etc.) and time of day
[behaviour type X time of day]
    5:00pm dog cries happily (owner home from work)
    3:00am sleeping


1010101010
** Regularization for Simplicity
generalize: not trusting your examples too much
regularization is what we do to avoid overfitting
    1. stop early, stop before on convergint training data
        difficult to do on practice, but often used
    2. penalize model complexity
        a. empirical risk minimization
        b. structural risk minimization
        balance these
define model complexity
    1. prefer smaller weights
        ridge regularization or L2 regularization, penalize the sum of the squared values of the weights
        pays attention to training data but makes sure our weights are sort of not bigger than they need to be
        loss
        Î» scalar value that controls how weights are balanced, which means getting the examples right vs/ making the model simple
        w1**2 + ... + wn**2: square of L2 norm


** Regularization for Simplicity: Lâ Regularization
regularization, penalizing complexity
minimize(loss(data|model))
minimize(loss(data|model) + complexity(model))
    this is the loss term and the regularization term
This course focuses on two common ways to think of complexity
    model complexity as a function of weights of all features in the model
    model complexity as a function of the total number of features with nonzero weights
L2 regularization
    sum of the squares of all the feature weights
    w1**2 + w2**2 + w3**2 ...

** Regularization for Simplicity: Lambda
lambda = regularization rate
    l2 regularization has:
        encourages weight values toward 0 (but not exactly 0)
        encouraged the mean of the weights toward 0
increasing lambda strengthens regularization effect
if your lambda value is too high your model will be simple, but you run the risk of underfitting. your model wont learn enough about the training data to make useful predictions
too low and model will be more complex, overfitting
lambda at 0 removes regularizartion
lambda and learning rate have a close connection
strong l2 drives feature weights to 0, lower learning rates (with early stopping) often produce often produce the same effect b/c the steps away from 0 aren't as large
making learning rate/lambda simultaneously have confounding effects

** UI Exercise
with small noisy training sets overfitting is a real concern


1111111111
** Logistic Regression
logistic regressions generates a probability, a value between 0 and 1
if the model says 0.932--93.2% then that much will actually be spam (in the spam example)

** Logistic Regression: Calculating a Probability
sigmoid gives value between 0 and 1
    there's asymptotes so it never hits 0 or 1, because of these we need regularization
if we need non-linearities we can by cross product
p(bark|night)
startled = p(bark|night) * nights
    = 0.05 * 365
    = 18
y' = sigmoid 1 / (1 + e**-z)
z = log(y / 1 - y)

** Logistic Regression: Loss and Regularization
regularization is extremely important in logistic regression modelling b/c without it the asymptotic nature of logreg would keep driving loss toward 0
most logistic regression models use:
    L2 regularization
    early stopping, limiting number of steps/learning rate


1212121212
** Classification
logistic regression as a foundation for classification, by taking our probability outputs and applying a fixed threshold to them: example
    if spam probability greater than 0.8, 0.8 is our classification threshold
performance? accuracy, all-right / all-wrong
    has some key flaws, breaks down with class imbalance
    if our click-through rate is 1 in 10000, our accuracy is 99.999% which has no value

True positives | False Positives
--------------------------------
False Negatives| True Negatives

The little boy who cried wolf

    True Positive: We correctly called the wolf! We saved the town
    False Positive: We called the wolf falsely, everyone is mad
    False Negative: There was a wolf, but we didn't spot it. it ate all our chickens
    True Negatives: No wolf, no alarm. everone is fine

precision (true positives)/(all positive predictions): how many times was the boy right?
recall (true positives)/(all actual positives): of all the wolves that attempted to get into the village, how many did we get?
when given precision value, you must ask for recall before meaning
if you raise the classification threshold of 1 and 0 precision will definitely increase
ROC curve, Receiver Operating Characteristic s curve
ROC Curve with 2 random values 1 positive and 1 negative
    probability is equal to the number under the ROC curve
prediction bias, average of predictions == average of observations
    we would like prediction to be observed, if not we has it has some bias
    bias of 0 shows avg pred == avg observation
bias is easy to fool
if bias isn't 0 then something is going on--can help determine debugging
0 bias does not mean model is perfect

** Thresholding
model returns 0.9995 mail predicting spam
another email msg w/ predition score of 0.0003, likely not spam
what about email of score 0.6?
in order to map a logistical regression value to binary you must define a classification threshold aka decision threshold
A value above indicated spam, a value below not spam
tempting to make 0.5 but thresholds are problem dependant
tuning: making non-spam as spam is bad, but spam as non-spam isn't as bad

** Classification: Accuracy
accuracy = num-correct-pred/total-num-pred
accuracy = (TP + TN)/(TP + TN + FP + FN) 
    True Positive, True Negative, False Positive, False Negative
class-imbalanced data set, wher there is significant disparity between the number of pos and neg labels
if 91/100 correct there are still some problem in the wolf criers village

** Classification: Precision and Recall
precision = TP / TP + FP
    1 / 1 + 1 = 0.5 (1 FP) 50% accuracy
recall = TP / TP + FN
    1 / 1 + 8 = 0.11 (8 FN) 11% are tumours

improving precision reduces recall & vice-versa
TN TN TN FN FN | FP TP TP FP
-- -- -- -- -- | -- -- -- --
               |
classification threshold

precision = TP / (TP + FP)
recall = TP (TP + FN)

moving the classification threshold will yeild diferent stats


** Classification: ROC Curve and AUC
ROC curve (receiver operating characteristics curve)
    plots two parameters, TP and FP
true positive rate (TPR) TPR = TP / (TP + FN) --recall
false positive rate (FPR) FPR = FP / (FP + TN)

AUC (Area under the ROC curve)
AUC ranges from 0 to 1
    AUC is scale-invariant, measures how pred are ranked
    AUC is classification-threshold-invariant, it measures the quality of the model's predictions irrespecrive of what classification threshold is chosen

    scale invariance is not always desirable, sometime we need calibrated probablity, AUC won't tell us that
    classification-threshold invariance is not always desireable, example: minimizing false positives (like in spam)

** Classification: Prediction Bias
prediction bias is a quanitity that measures how far those two averages are
    pred-bias = avg-pred / avg-labels-in-set
    wx + b
causes of prediction bias:
    incomplete feature set
    noisy data set
    buggy pipeline
    biased training sample
    overly strong regularization
post-processing, adding a calibration layer
    if your model has a +3% bias you could add a calibration layer to reduct pred bias
    bad idea though:
        fixing the sympton rather than the cause
        more bittle must kept up to date
a good model will usualy have 0 pred-bias, this doesn't mean your model is good

logistic reg preducts between 0 and 1
all label examples are either exatly 0 or 1
you cannot accurately predict bias on onnly 1 example, need a "bucket" of examples
must have enough examples
can form buckets in 2 ways
    linearly breaking up the target pred
    forming quantiles
why are there inaccuracys?
    the training set doesn't adequately represent certain subsets of the data space
    some subsets of the data are noisier than others
    the model is overly regularized

** Binary Classification Exercise
if one feature's range spans 500 to 100,000 and another feature spans 2 to 12, the model will be difficult/impossible to train, therefore you normalize features

train_df_norm = (train_df - train_df.mean()) / train_df.std()
same with test
test_df_norm = (test_df - test_df.mean()) / test_df.std()


1313131313
** Regularization for Sparsity
cross features, can be great but can cause some problems too
    sparse features
0 out weights, to reduce noise
L0 regularization, penalize for having a weight that's not 0
so we relax L0 to L1, penalizes the sum of the absolute value of the weights
L2 regularization, tries to make the weight small but won't drive them all the way to 0

** Regularization for Sparsity: L1 Regularization
sparse vectors often contain many dimensions, creating a feature cross results in even more dimensions. many vectors is more RAM
drop weights to 0 where possible
0 weight remooves feature from model, 0's save ram
    scanning for houses, don't scan ocean
L1 penalizes weight**2
L2 penalizes |weight|
derivative of L2 is 2 * weight
    you can think of the derivative of L2 as a force that removes x% of the weight every time
derivative of L1 is k (a constant whose value is independent of weight)
    you can think of the derivative of L1 as a force that subtracts some constant from the weight every time. Thanks to absolute values L1 has disconuity at 0, example: +0.1 to -0.2 L1 set the weight to 0

NOTE L2 approaches limit of 0, L1 reaches 0


** Understanding
L1 may cause informative features to get a weight of exactly 0
L1 will produce a smaller model


1414141414
** Neural Networks
back propagation
activation functions

** Neural Net Spiral Solution
regularization penalizes complex models and overfitting\feature engineering can help


1414141414
** Training Neural Networks
Don't need to know back propogation
back prop relies on gradients, things need to be derivatives
gradients can vanish if networks get too deep, so if signal to noise ratios get bad down the model learning can become slow
limit depth of model to the minimum effective depth if you can
gradients can explode; if learning rates are too high we can get crazy instabilities. try lowering learning rate
ReLu's can die -- keep lower learning rates
normalization useful
useful trick for nn's a regularizarion called dropout
    drop out a random x, and then another random x

** Training Neural Networks: Best Practices
failure cases of back propogation
    vanishing gradients
        lower levels (closer to input) can become very small
        when gradient approaches 0 for lower layers they train slow
        relu helps prevent this
    exploding gradients
        low layers have many large items, gradients get too large to converge
        batch normalization and lower learning rate help stop this
    dead relu units
        sum for relu falls below 0, relu can get stuck
        lower learning rate
Dropout regularization
    "dropping out" unit for a single gradient step, the more you drop the stronger the regularizartion
        0.0 = no dropout reg
        1.0 = drop out everything
        values between 0.0 and 1.0 are more useful


1515151515
** Multi-Class Neural Networks
multi-class classification (dog is beagle, basset hound, or blood hound)
softmax: probability of a class where probabilities add up to 1.0: dog 0.9, cat 0.08, horse 0.02
    candidate sampling, where we train output nodes for the class is belongs to and then take a sample of the negative classes and update a sample of the output nodes
of it's broader its a 1 vs. all classification strategy

** Multi-Class Neural Networks: One vs. All
one vs all, way to leverage binary classification. N solutions, N seperate binary classifiers.
    example, pic of a dog, five different recognizers may be trained, 4 neg ex of dog, 1 pos example is a dog
    inefficient as the number of classifiers increase

** Multi-Class Neural Networks: Softmax
recall log reg is 0-1.0, log reg output of 0.8 is 80%, therefore 20% false
softmax extends this idea to multi-class
    0.001 apple
    0.04 bear
    0.008 candy
same number of nodes as output layer
full softmax, every possible class
candidate sampling, probability for all positive labels and only 1 random negative sample. interested in determining whether image is beagle or bloodhound, don't provide examples for every non-doggy sample
softmax assumes each example is a member of exactly 1 class,
1 member of multiple classes?
    may not use softmax
    must rely on multiple log reg


1616161616
** Embeddings
embedding is a relatively low-dimenasional space which you can translate high-dimensional vectors. These make ml easier to do on large inputs such as sparse vectors representing words. captures semantics of the input by placing semantically similar inputs close together in the embedding space. an embedding can be learned and reused across models

** Embeddings: Motivation From Collaborative Filtering
collaborative filtering is the task of making predictions about the interests of a user based on many other users' interest
to solve this we use embedding
one-dimensional arrangement, just a line vs
two-dimensional arrangement which is clusters
embedding space (2d vector, n vector)
latent dimension, feature for the dimension (axis)

** Embeddings: Categorical Input Data
categorical data refers to features that represent 1 or more discrete items from a finite set of choices
catagorical data is most effeciently represented via sparse tensors, tensors with very few non-zero elements
recommending movies:
    assign an ID to each movie
    [1, 3, 999999] (preprocessing)
representations used to sparse vectors are given to give depth to data
id 1247 is a one-hot encoding
also there are vectors wil 500,000 nodes w/ non-0 value
Size of network, M wods, N nodes, MxN weights to train
a large number of weights causes futher problems:
    amount of data, more weights, more data to train effectively
    amount of computation, the more weights, the more computation required to train and use model. easy to exceed hardware
id's aren't catagorical, they have no commonality. solution: embeddings, they translate large sparse vectors into a lower-dimensional space that preserves semantic relationships

** Embeddings: Translating to a Lower-Dimensional Space
an embedding is a matric which each column is the vector that corresponds to an item in your vocabulary. to get the dense vector for a single vocab item you retrieve the column corresponding to that item
representing multiple vocab? retrieve embedding for all items and add them together
1 x N sparse representation, S and N x M embedding table E, the matrix multiplication S x E gives you the 1 x M dense vector. How do you get E though?

** Embeddings: Obtaining Embeddings
Word2vec "you shall now a word by the company it keeps"
embedding as part of the tasks nn, could take longer then using training for embedding






