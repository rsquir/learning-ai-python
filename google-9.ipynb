{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae7c04d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported the modules.\n"
     ]
    }
   ],
   "source": [
    "#@title Load the imports\n",
    "\n",
    "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# The following lines adjust the granularity of reporting.\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "\n",
    "print(\"Imported the modules.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "436a906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\")\n",
    "test_df = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\")\n",
    "\n",
    "# Scale the labels\n",
    "scale_factor = 1000.0\n",
    "# Scale the training set's label.\n",
    "train_df[\"median_house_value\"] /= scale_factor \n",
    "\n",
    "# Scale the test set's label\n",
    "test_df[\"median_house_value\"] /= scale_factor\n",
    "\n",
    "# Shuffle the examples\n",
    "train_df = train_df.reindex(np.random.permutation(train_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07fa4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty list that will eventually hold all feature columns.\n",
    "feature_columns = []\n",
    "\n",
    "# Create a numerical feature column to represent latitude.\n",
    "latitude = tf.feature_column.numeric_column(\"latitude\")\n",
    "feature_columns.append(latitude)\n",
    "\n",
    "# Create a numerical feature column to represent longitude.\n",
    "longitude = tf.feature_column.numeric_column(\"longitude\")\n",
    "feature_columns.append(longitude)\n",
    "\n",
    "# Convert the list of feature columns into a layer that will ultimately become\n",
    "# part of the model. Understanding layers is not important right now.\n",
    "fp_feature_layer = layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73f209b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_model() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d0e2b3df294f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create and compile the model's topography.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_feature_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Train the model on the training set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: build_model() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.05\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "label_name = 'median_house_value'\n",
    "\n",
    "# Create and compile the model's topography.\n",
    "my_model = create_model(learning_rate, fp_feature_layer)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n",
    "\n",
    "plot_the_loss_curve(epochs, rmse)\n",
    "\n",
    "print(\"\\n: Evaluate the new model against the test set:\")\n",
    "test_features = {name:np.array(value) for name, value in test_df.items()}\n",
    "test_label = np.array(test_features.pop(label_name))\n",
    "my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24db9b37",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resolution_in_degrees = 1.0 \n",
    "\n",
    "# Create a new empty list that will eventually hold the generated feature column.\n",
    "feature_columns = []\n",
    "\n",
    "# Create a bucket feature column for latitude.\n",
    "latitude_as_a_numeric_column = tf.feature_column.numeric_column(\"latitude\")\n",
    "latitude_boundaries = list(np.arange(int(min(train_df['latitude'])), \n",
    "                                     int(max(train_df['latitude'])), \n",
    "                                     resolution_in_degrees))\n",
    "latitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column, \n",
    "                                               latitude_boundaries)\n",
    "feature_columns.append(latitude)\n",
    "\n",
    "# Create a bucket feature column for longitude.\n",
    "longitude_as_a_numeric_column = tf.feature_column.numeric_column(\"longitude\")\n",
    "longitude_boundaries = list(np.arange(int(min(train_df['longitude'])), \n",
    "                                      int(max(train_df['longitude'])), \n",
    "                                      resolution_in_degrees))\n",
    "longitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column, \n",
    "                                                longitude_boundaries)\n",
    "feature_columns.append(longitude)\n",
    "\n",
    "# Convert the list of feature columns into a layer that will ultimately become\n",
    "# part of the model. Understanding layers is not important right now.\n",
    "buckets_feature_layer = layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f89c6207",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0ab7a217f902>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Build the model, this time passing in the buckets_feature_layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuckets_feature_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train the model on the training set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_model' is not defined"
     ]
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.04\n",
    "epochs = 35\n",
    "\n",
    "# Build the model, this time passing in the buckets_feature_layer.\n",
    "my_model = create_model(learning_rate, buckets_feature_layer)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n",
    "\n",
    "plot_the_loss_curve(epochs, rmse)\n",
    "\n",
    "print(\"\\n: Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08db4c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution_in_degrees = 1.0 \n",
    "\n",
    "# Create a new empty list that will eventually hold the generated feature column.\n",
    "feature_columns = []\n",
    "\n",
    "# Create a bucket feature column for latitude.\n",
    "latitude_as_a_numeric_column = tf.feature_column.numeric_column(\"latitude\")\n",
    "latitude_boundaries = list(np.arange(int(min(train_df['latitude'])), int(max(train_df['latitude'])), resolution_in_degrees))\n",
    "latitude = tf.feature_column.bucketized_column(latitude_as_a_numeric_column, latitude_boundaries)\n",
    "\n",
    "# Create a bucket feature column for longitude.\n",
    "longitude_as_a_numeric_column = tf.feature_column.numeric_column(\"longitude\")\n",
    "longitude_boundaries = list(np.arange(int(min(train_df['longitude'])), int(max(train_df['longitude'])), resolution_in_degrees))\n",
    "longitude = tf.feature_column.bucketized_column(longitude_as_a_numeric_column, longitude_boundaries)\n",
    "\n",
    "# Create a feature cross of latitude and longitude.\n",
    "latitude_x_longitude = tf.feature_column.crossed_column([latitude, longitude], hash_bucket_size=100)\n",
    "crossed_feature = tf.feature_column.indicator_column(latitude_x_longitude)\n",
    "feature_columns.append(crossed_feature)\n",
    "\n",
    "# Convert the list of feature columns into a layer that will later be fed into\n",
    "# the model. \n",
    "feature_cross_feature_layer = layers.DenseFeatures(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22ba13c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-32f751c80eeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Build the model, this time passing in the feature_cross_feature_layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_cross_feature_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train the model on the training set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_model' is not defined"
     ]
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.04\n",
    "epochs = 35\n",
    "\n",
    "# Build the model, this time passing in the feature_cross_feature_layer: \n",
    "my_model = create_model(learning_rate, feature_cross_feature_layer)\n",
    "\n",
    "# Train the model on the training set.\n",
    "epochs, rmse = train_model(my_model, train_df, epochs, batch_size, label_name)\n",
    "\n",
    "plot_the_loss_curve(epochs, rmse)\n",
    "\n",
    "print(\"\\n: Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=test_features, y=test_label, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09c4e528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumericColumn(key='a', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)\n"
     ]
    }
   ],
   "source": [
    "data = {'a': [15, 9, 17, 19, 21, 18, 25, 30],\n",
    "   'b': [5.0, 6.4, 10.5, 13.6, 15.7, 19.9, 20.3 , 0.0]}\n",
    "a = tf.feature_column.numeric_column('a')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6afc47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
